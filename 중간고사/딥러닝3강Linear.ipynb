{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"딥러닝3강Linear.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmPmd0GrDJ2mqOMNy16lwB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"M2JeAIbmas8L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3R3vsKumcFCz"},"source":["#Linear Models - 선형모델 #\n","\n","- 딥러닝 전 전통적인 모델이다. \n","###1.Linear Regression###\n","- 선형 회귀는 입력 받은 변수들의 조합(특징분석)을 통해서 타겟 변수를 예측한다. \n",">$\\hat{y}=w_1x_1+ w_2x_2+ ...+ b$\n","\n","- Vector notation \n",">${y} = wx + b$\n","\n","실제로 학습하는 것은 각 feature $x_1, x_2...$에 대한 $w_1, w_2, ...$ 이다. 따라서 $w와 b$를 찾아내는 것이 학습이다. \n","\n","이때 현재는 __Regression__을 하고 있기 때문에 $y$는 ($y_1,y_2, ...$) 각 샘플의 정답 : 정답 자체의 value를 나타낸다. \n","\n","-> 각 샘플들의 feature에 $w_i$를 곱해서 다 합치면 $y_i$가 나와용 | $w$는 각 feature마다 대응 하는 $w$-value를 가지고 있다. (과제 3번에서 와인 feature 마다 있던 것) \n","\n","\n","- Loss function (cost function) in Linear Regression\n",">$Loss_{linear} = \\frac{1}{2N} \\sum_i{(\\hat{y}-y)^2}$\n","(least square)\n","\n","- 예측 값에서 실제 값을 빼주고 다 더한뒤 모수로 나누면 해당 값이 loss function의 값이 된다. 결론적으로 이 값을 0에 가깝게 만들어주면 회귀 예측의 정확도가 상승한다.\n","- 해당 lossfucntion에 $argmin_w[f(x)]$ 를 씌워 가장 값이 작은 쪽에서 기울기 ($w$ : 가중치)를 선택하면서 w의 값을 최적화 한다. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"gQwaV-1FEkSB"},"source":["# Exercises\n","\n","Apply linear regression to sigmoid function\n","\n",">$sigmoid(x) = \\frac{1}{1+exp(-x)}$\n","\n","\n","## Data generation\n","\n","1. generate X as an array\n","```\n","    X = np.arange(-10., 10., 0.2)\n","    X = np.expand_dims(X, axis = 1)\n","    \n","```\n","\n","2. generate y as using sigmoid() and np.random.normal() functions\n","\n","```   \n",">>  y = sigmoid(X[:, 0]) + np.random.normal(0, 0.1, size = [len(X)])\n","```\n","\n","\n","* Check shape of X and y\n","\n","\n","\n","## Linear regression\n","\n","1. Split data into training set and test set\n","\n","2. import LinearRegression\n","\n","3. Create a LinearRegression object\n","\n","4. Train  (call fit() function )\n","\n","5. Apply linear regression (call predict() function)"]},{"cell_type":"markdown","metadata":{"id":"1afSwVmIr2Zx"},"source":["#Bias - Variance Trade_off \n","\n","- $Error(X)= noise(X) + bias(X) + variance(X)$ \n","- $noise$는 데이터가 가지는 본질적인 한계치이기 때문에 다룰 수가 없음 \n","\n","\n","$Bias$\n","\n"," 데이터 내에 있는 모든 정보를 고려하지 않음으로(편향) 인해, 지속적으로 잘못된 것들을 학습하는 경향을 말하며, underfitting이라고 함 \n","\n","$Variance$\n","\n"," 데이터 내에 있는 에러나 노이즈까지 잘 잡아내는 highly flexible models에 데이터를 fitting 시킴으로써, 실제 현상과 관계 없는 random한 것들까지 학습하는 알고리즘의 경향을 의미한다. 이는 overfitting과 관계가 있음 \n","\n","\n","\n","\n","- Model Complexity에서 에러는 두종류가 있다. $Variance + Bias^2 = total Error$ 인데, 해당 total Error의 기울기가 가장 낮은 곳에서 $optimum model complexity$(최적)이 나온다. \n","\n","\n","1. Underfitting \n","\n","\n","- train에 대한 성능도 __낮음__, 따라서 test에 대한 성능도 __낮을 수밖에__ 없음 \n","- Bias(편향)는 under-fitting과 관련이 있다.\n","\n","2. Overfitting \n","\n","\n","- tarin에 대한 성능이 __매우 높음__, 학습에 대한 정답률이 매우 높아서 학습한 데이터에 대해서만 잘함 따라서 test에 대한 성능이 __낮음__ \n","- variance는 overfitting에 연관이 있다. \n","\n","3. 모델에 대한 복잡도 \n","\n","\n","- 모델이 복잡할 수록 variance(분산: 흩어진 정도)에 대한 에러가 __높음__\n","- 모델이 단순할 수록 Bias에 대한 에러가 __높다__\n","- 적정모델은 train data에 대해서 제일 잘하는 모델은 아님\n","- 그래프가 $Bais^2$의 값이 커질수록 error가 작아지고, $Variance$의 값이 작을 수록 error가 작아진다. 해당 값들은 Model Complexity의 다른 표현이기도 하다.\n","- model이 너무 단순하거나 너무 복잡할 수록 에러의 값이 커진다.\n","\n","\n","4.cf \n","\n","- 방정식을 풀기 위한 점의 갯수가 모자란 경우 => 해가 엄청 많아서 overfitting(?)\n","- 모델이 복잡하여 파라미터가 많은 경우 train 데이터와의 차이가 클 수록 (overfitting) variance 문제 발생 \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wg7qlYLC1PZu"},"source":["#Regularized Regression#\n","- 오버 피팅을 해결하기 위한 방법 : 정규화!\n","- 변수(feature)의 갯수를 줄이지 않고, 데이터를 보존하면서 모델의 복잡도(차수)를 잡아주는 방법으로 regularization을 선택, loss function에 조정을 위한 항을 새로 추가함 \n","- regularized는 앞에 $a$라는 상수가 붙는데, 이는 값을 1보다 작게 설정을 해준다. 그 이유는 메인 에러가 아니기 때문에 같은 비중으로 다루어지면 안되기 때문이다. \n","\n","1. Ridge regression\n","####- linear regression + __$L_2$ regularization__\n","- 제곱을 통해 길이를 최소화 한다. => 제곱을 하기 때문에 큰 값에 패널티가 크게 들어가서 값이 큰 weight가 손해를 본다. \n","- 전체적으로 feature가 plane 해지는 경향이 있다. \n","2. Lasso regression\n","####- linear regression + __$L_1$ regularization__\n","- 절대 값의 합 ==> 모든 값에 대해서 동일한 패널티를 부여한다. 따라서 w 값이 작을 수록 zero가 많이 된다. feature 값이 클 수록 생존하며, 살아 있는 요소들을 좋은 feature로 판단한다. ( 0 이라는 것은 해당 특징을 사용하지 않는다는 것이다 ) \n","- 대부분의 element가 0이다. sparse 하다(살아 있는 친구들의 희소하다)고 교수님이 하심. \n","3. Elastic-Net\n","####- linear regression + __$L_1$ regularization__ + __$L_2$ regularization__\n","- 두가지의 특징을 병행하는 방법이다. \n","\n","\n","\n","4. cf\n","- 데이터가 많을때 regularization이 필요가 없으므로 알파값이 감소한다.\n","- 알파값은 모델을 단순화 시켜주기 때문에 복잡한 문제에는 복잡한 모델이 필요함으로 알파값을 감소 시켜준다. \n","- 따라서 단순한 문제에 대해서 너무 복잡한 모델이 나타난다면 알파값을 조정하여 모델을 좀더 단순화 시켜준다. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"4BGdnSc05BtE","cellView":"code","executionInfo":{"status":"ok","timestamp":1603639953373,"user_tz":-540,"elapsed":846,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":["from sklearn.datasets import load_wine\n","wine_dataset = load_wine()\n","\n","import matplotlib.pyplot as plt \n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(wine_dataset['data'],wine_dataset['target'],random_state=0)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeRLCg-Vr0lr","executionInfo":{"status":"ok","timestamp":1603639953378,"user_tz":-540,"elapsed":842,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}},"outputId":"270ea2c7-f73b-45c9-adca-977aafb9c61e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["from sklearn.linear_model import LinearRegression \n","\n","lr = LinearRegression().fit(X_train, y_train)\n","# TO DO: check coeeficients and intercept\n","print(\"w :\",lr.coef_)\n","print(\"b :\",lr.intercept_) # 여기에서 B는 regression이기 때문에 값이 하나로 나옴 \n","\n","y_hat = lr.predict(X_test)\n","lrY = lr.score(X_test,y_test)\n","# plt.plot(y_test, y_hat, 'o')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["w : [-8.50242320e-02  2.64695918e-02 -8.83765299e-02  3.40557095e-02\n"," -3.47739383e-04  2.24005481e-01 -4.71313726e-01 -4.32777162e-01\n","  8.20042826e-02  5.34671385e-02 -1.20953501e-01 -3.17091613e-01\n"," -6.70222507e-04]\n","b : 3.2039134848291004\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxbW-Hz95Txy","executionInfo":{"status":"ok","timestamp":1603639953380,"user_tz":-540,"elapsed":839,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":["from sklearn.linear_model import Ridge\n","rigde = Ridge(alpha=0.1).fit(X_train, y_train)\n","yrd_hat = rigde.predict(X_test)\n","rgY = rigde.score(X_test,y_test)\n","# plt.plot(y_test, yrd_hat, 'o')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdhiIdIN5l09","executionInfo":{"status":"ok","timestamp":1603639953381,"user_tz":-540,"elapsed":774,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":["from sklearn.linear_model import Lasso\n","lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n","yls_hat = lasso.predict(X_test)\n","lasY = lasso.score(X_test,y_test)\n","# plt.plot(y_test, yls_hat, 'o')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3RY6t2T5vJ5","executionInfo":{"status":"ok","timestamp":1603639953382,"user_tz":-540,"elapsed":615,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":["from sklearn.linear_model import ElasticNet\n","elastic = ElasticNet(alpha=0.001, l1_ratio=0.5).fit(X_train,y_train)\n","yel_hat = elastic.predict(X_test)\n","elaY = elastic.score(X_test,y_test)\n","# plt.plot(y_test, yel_hat, 'o')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_f_R4uws6AQP","executionInfo":{"status":"ok","timestamp":1603639953744,"user_tz":-540,"elapsed":729,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}},"outputId":"68dfd443-cc15-4d70-cf8e-11c1c5f72fa2","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(\"LinearRegression : \",lrY)\n","print(\"Ridge : \",rgY)\n","print(\"Lasso : \",lasY)\n","print(\"ElasticNet : \",elaY)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["LinearRegression :  0.8043532631769539\n","Ridge :  0.8062350353487191\n","Lasso :  0.8298747376836272\n","ElasticNet :  0.8077652618647688\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kg7elmaw6iif","executionInfo":{"status":"ok","timestamp":1603639953744,"user_tz":-540,"elapsed":623,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":["from sklearn.linear_model import Ridge\n","from sklearn.linear_model import RidgeClassifier\n","# Regression 모델을 그냥 불러오면 regression이고, 그 뒤에 Classifier를 붙이면 classifier임 "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXBeerx49_g5","executionInfo":{"status":"ok","timestamp":1603639953745,"user_tz":-540,"elapsed":524,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}}},"source":[""],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gj1ruosJ9_7j"},"source":["#Logistic Regression \n","- Logistic regression = linear regression + __$logisitic function$__\n","- $Logistic(x) = sigmoid(x) = \\frac{1}{1+exp(-x)}$\n","- __Class__에 속할 확률을 구하고 싶을 때 Logistic function에 넣어버린다. \n","\n","\n","1. Discrimination function\n","- linear regression의 결과를 logistic function에 넣어버리면 각 클래스에 속할 확률이 나온다. 때문에 범위는 [0, 1]\n","- $f_i(x) = logistic(wx+b)$ \n","\n","\n","2. Parameter estimation \n","- 동일하게 $W^*$는 $argmin_wL(\\hat{Y},Y;W)$\n","- $L(')$ 는 loss function으로 cross entropy를 사용하거나 MSE를 사용한다. \n"]},{"cell_type":"markdown","metadata":{"id":"rnPVbI4G_WuR"},"source":["#Loss Functions \n","\n","##1. MSE ; Mean Squared Error \n","- 선형 모델과 매우 매우 유사하다. \n","-$E =  \\frac{ \\sum_N{(\\hat{y_t} - y_t)^2}}{N} $\n","\n","\n","## 2. Cross entropy (with softmax activation)\n","- Softmax activation : $\\hat{y_t} = \\frac{exp(net_t)}{\\sum_nexp(net_t)}$ (확률의 성질을 가진다. 정답과 근사하지만 1.0로 일치하지는 않음 \n","\n","- $E = - \\sum_N y_t log(\\hat{y_t})$\n","\n","\n","\n","- 확률의 성질을 사용하지 못하면 Cross entropy 사용할 수 없다. 일반적인 regression은 [0, 1]에만 값이 포함되는 경우는 거의 없기 때문. \n","- 하지만 CE를 활용한 loss function은 MSE 보다 CLassification에서 월등한 성능을 보여준다. \n","\n"]},{"cell_type":"code","metadata":{"id":"g-4VgvRH_U1_","executionInfo":{"status":"ok","timestamp":1603639955424,"user_tz":-540,"elapsed":933,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}},"outputId":"fa9b8289-7013-4e56-ad4d-0680dc169c91","colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["#LogisticRegression in scikit-learn\n","from sklearn.linear_model import LogisticRegression\n","logi_reg = LogisticRegression().fit(X_train,y_train)\n","\n","print(\"logi_reg.coef_ :\", logi_reg.coef_) # w\n","print(\"logi_reg.intercept_:\", logi_reg.intercept_) # b  : 클래스의 수대로 나옴 \n","\n","print(\"score :\",logi_reg.score(X_test,y_test))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["logi_reg.coef_ : [[-1.32186429e-01  3.23965050e-01  1.84092284e-01 -2.58932260e-01\n","  -3.55944236e-02  2.38697315e-01  5.82114322e-01 -2.22946486e-02\n","   9.92276027e-02  1.09948852e-01 -1.73973402e-02  4.10341111e-01\n","   8.36031752e-03]\n"," [ 5.99095455e-01 -7.17070268e-01 -2.00230225e-01  2.11071050e-01\n","  -1.04352065e-02  2.06746427e-01  3.95799510e-01  1.36191938e-02\n","   3.85959371e-01 -1.24982865e+00  2.65765529e-01  4.38399876e-01\n","  -8.01466640e-03]\n"," [-4.66909026e-01  3.93105218e-01  1.61379407e-02  4.78612099e-02\n","   4.60296301e-02 -4.45443742e-01 -9.77913832e-01  8.67545481e-03\n","  -4.85186974e-01  1.13987979e+00 -2.48368189e-01 -8.48740987e-01\n","  -3.45651121e-04]]\n","logi_reg.intercept_: [-0.04937168  0.10908187 -0.05971019]\n","score : 0.9333333333333333\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_t0DCTC-DFDZ","executionInfo":{"status":"ok","timestamp":1603640425943,"user_tz":-540,"elapsed":1109,"user":{"displayName":"안수현학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiflJXlQYn878OGBH0D279WP8bga5RYOxISvAom0g=s64","userId":"09538039103302826305"}},"outputId":"8441fead-145c-41e2-fd9a-30fdf462394c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for value in [0.01, 0.1, 1, 10, 100]:\n","  logi_reg = LogisticRegression(C=value).fit(X_train,y_train)\n","  print(\"C = {:4} | train score :{:5.5}\".format(value, logi_reg.score(X_train,y_train)))\n","\n","for value in [0.01, 0.1, 1, 10, 100]:\n","  logi_reg = LogisticRegression(C=value).fit(X_train,y_train)\n","  print(\"C = {:4} | test score :{:5.5}\".format(value, logi_reg.score(X_test,y_test)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["C = 0.01 | train score :0.93985\n","C =  0.1 | train score :0.96992\n","C =    1 | train score :0.99248\n","C =   10 | train score :0.98496\n","C =  100 | train score :0.98496\n","C = 0.01 | test score :0.95556\n","C =  0.1 | test score :0.95556\n","C =    1 | test score :0.93333\n","C =   10 | test score :0.91111\n","C =  100 | test score :0.91111\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2TOz2WpNY30C"},"source":[""],"execution_count":null,"outputs":[]}]}